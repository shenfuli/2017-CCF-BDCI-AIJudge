{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/xiaosa_kun/article/details/84868437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/ML_Study/2017-CCF-BDCI-AIJudge\")\n",
    "import pandas as pd\n",
    "from config.db_config import Config\n",
    "import numpy as np\n",
    "from utils import LOGGER\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.data_csv_path = \"D:/ML_Study/2017-CCF-BDCI-AIJudge/data/output/corpus/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>laws</th>\n",
       "      <th>penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>公诉 机关 霍邱县 人民检察院 被告人 许某 1975 日生 2012 因涉嫌 危险 驾驶 ...</td>\n",
       "      <td>133,72,73</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>公诉 机关 海口市 龙华区 人民检察院 被告人 王某 海口市 龙华区 人民检察院 海龙 检公...</td>\n",
       "      <td>347,67,52,64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>公诉 机关 广东省 潮州市 人民检察院 被告人 覃学彬 1980 出生 广西壮族自治区 大新...</td>\n",
       "      <td>263,25,52,53,55,56</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>公诉 机关 榆林市 榆阳区 人民检察院 上诉人 原审 被告人 2012 因涉嫌 盗窃罪 榆林...</td>\n",
       "      <td>264,52,53,67,72,73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>公诉 机关 榆阳区 人民检察院 上诉人 原审 被告人 刘某 汉族 陕西省 横山县 小学文化 ...</td>\n",
       "      <td>224,25,26,27,52,72</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            content                laws  \\\n",
       "0  16  公诉 机关 霍邱县 人民检察院 被告人 许某 1975 日生 2012 因涉嫌 危险 驾驶 ...           133,72,73   \n",
       "1  32  公诉 机关 海口市 龙华区 人民检察院 被告人 王某 海口市 龙华区 人民检察院 海龙 检公...        347,67,52,64   \n",
       "2  41  公诉 机关 广东省 潮州市 人民检察院 被告人 覃学彬 1980 出生 广西壮族自治区 大新...  263,25,52,53,55,56   \n",
       "3  57  公诉 机关 榆林市 榆阳区 人民检察院 上诉人 原审 被告人 2012 因涉嫌 盗窃罪 榆林...  264,52,53,67,72,73   \n",
       "4  60  公诉 机关 榆阳区 人民检察院 上诉人 原审 被告人 刘某 汉族 陕西省 横山县 小学文化 ...  224,25,26,27,52,72   \n",
       "\n",
       "   penalty  \n",
       "0        3  \n",
       "1        1  \n",
       "2        5  \n",
       "3        5  \n",
       "4        7  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(config.data_csv_path,encoding='utf-8',sep=',')\n",
    "train_df = train_df.dropna()\n",
    "train_df['penalty'] = train_df['penalty'].astype(int)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看训练集每个分类的名字以及样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 36\n",
      "2 50\n",
      "3 18\n",
      "4 14\n",
      "5 16\n",
      "6 12\n",
      "7 48\n",
      "8 6\n"
     ]
    }
   ],
   "source": [
    "# 类别：数据不均衡，需要进行采样 to do\n",
    "for name,group in train_df.groupby(by='penalty'):\n",
    "    print(name,len(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in train_df['content']:\n",
    "    words_list.append(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['公诉', '机关', '霍邱县', '人民检察院', '被告人', '许某', '1975', '日生', '2012', '因涉嫌', '危险', '驾驶', '霍邱县', '公安局', '取保候审', '2013', '日经', '本院', '取保候审', '霍邱县', '人民检察院', '以霍检', '刑诉', '2013', '起诉书', '指控', '被告人', '许某', '甲犯', '危险', '驾驶', '2013', '日向', '本院', '提起公诉', '本院', '依法', '简易程序', '实行', '独任', '审判', '2013', '公开', '开庭审理', '本案', '霍邱县', '人民检察院', '检察员', '胡涛', '被告人', '许某', '到庭', '参加', '诉讼', '现已', '审理', '终结', '霍邱县', '人民检察院', '指控', '2012', '被告人', '许某', '酒后', '驾驶', '二轮', '摩托车', '沿霍寿路', '由南向北', '行驶', '霍寿路', '公园路', '交叉口', '路边', '行人', '相撞', '公安民警', '查获', '六安市', '疾病', '预防', '控制中心', '鉴定', '许某', '血液', '乙醇', '含量', '169.64', 'mg', '100ml', '上述事实', '被告人', '开庭审理', '过程', '无异议', '被害人', '杨正响', '陈述', '证人', '李某', '证言', '六安市', '疾病', '预防', '控制中心', '检验', '报告', '六安市', '疾控交', '检字', '2012', '155', '霍邱县', '公安局', '交通管理', '大队', '呼吸', '酒精', '检测', '抽取', '当事人', '血样', '登记表', '驾驶', '信息', '查询', '道路', '交通事故', '赔偿', '调解', '协议书', '经济', '赔偿', '凭证', '谅解', '被告人', '户籍', '信息', '证据', '证明', '足以认定'], ['公诉', '机关', '海口市', '龙华区', '人民检察院', '被告人', '王某', '海口市', '龙华区', '人民检察院', '海龙', '检公', '刑诉', '774', '起诉书', '指控', '被告人', '王某', '乙犯', '贩卖毒品', '日向', '本院', '提起公诉', '本院', '依法', '简易程序', '实行', '独任', '审判', '公开', '开庭审理', '本案', '海口市', '龙华区', '人民检察院', '指派', '检察员', '余荣标', '出庭', '支持', '公诉', '被告人', '王某', '到庭', '参加', '诉讼', '现已', '审理', '终结', '公诉', '机关', '指控', '时许', '被告人', '王某', '海口市', '龙华区', '龙昆', '北路', '温泉', '大酒店', '门前', '人民币', '550', '价格', '购毒', '人员', '王某', '贩卖', '小包', '毒品', '鉴定', '一小', '包含', '海洛因', '成分', '净重', '0.9980', '一小', '包含', '甲基苯丙胺', '成分', '净重', '1.0469', '交易', '公安民警', '抓获', '公诉', '机关', '被告人', '王某', '无视', '国家', '法律', '非法', '贩卖毒品', '海洛因', '甲基苯丙胺', '已触犯', '中华人民共和国', '刑法', '第三', '四十七', '第四款', '贩卖毒品', '追究其', '刑事责任', '提请', '本院', '依法', '判处', '被告人', '王某', '起诉书', '指控', '事实', '罪名', '持异议', '审理', '查明', '时许', '事先', '电话', '被告人', '王某', '海口市', '龙华区', '龙昆', '北路', '温泉', '大酒店', '门前', '人民币', '550', '价格', '购毒', '人员', '王某', '贩卖', '小包', '毒品', '交易', '公安民警', '抓获', '公安民警', '从王', '身上', '扣押', '毒资', '人民币', '550', '贩毒', '手机', '一部', '从王', '某甲', '身上', '扣押', '毒品', '小包', '鉴定', '一小', '包含', '海洛因', '成分', '净重', '0.9980', '一小', '包含', '甲基苯丙胺', '成分', '净重', '1.0469', '上述事实', '被告人', '王某', '开庭审理', '过程', '无异议', '到案', '通话', '清单', '证人', '王某', '证言', '辨认', '笔录', '扣押', '物品', '清单', '毒化', '鉴定书', '被告人', '王某', '常住人口', '信息', '在案', '供述', '证据', '证实', '足以认定']]\n"
     ]
    }
   ],
   "source": [
    "print(words_list[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 特征工程\n",
    "TfidfVectorizer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用sklearn.feature_extraction.text库的TfidfVectorizer方法实例化模型对象。\n",
    "#TfidfVectorizer方法需要4个参数。\n",
    "#第1个参数是分词结果，数据类型为列表，其中的元素也为列表；\n",
    "#第2个关键字参数stop_words是停顿词，数据类型为列表；\n",
    "#第3个关键字参数min_df是词频低于此值则忽略，数据类型为int或float;\n",
    "#第4个关键字参数max_df是词频高于此值则忽略，数据类型为Int或float。\n",
    "#查看TfidfVectorizer方法的更多参数用法，\n",
    "#官方文档链接：http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(words_list,min_df=3,max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 1343\n",
      "(200, 1343)\n"
     ]
    }
   ],
   "source": [
    "#第1行代码查看向量化的维数，即特征的维数；\n",
    "#第2行代码调用TfidfVectorizer对象的fit_transform方法获得特征矩阵赋值给X；\n",
    "#第3行代码查看特征矩阵的形状。\n",
    "X=tfidf.fit_transform(train_df['content'])\n",
    "print('词表大小:', len(tfidf.vocabulary_))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.26373434, 0.        ,\n",
       "        0.14269755]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 标签编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_df['penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用sklearn.preprocessing库的LabelEncoder方法对文章分类做标签编码。\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_df['penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 4, 4, 6, 0, 6, 1, 1, 1, 3, 7, 1, 7, 4, 1, 5, 0, 6, 6, 6, 1,\n",
       "       6, 2, 3, 1, 1, 6, 7, 1, 2, 6, 0, 6, 1, 3, 1, 6, 0, 0, 1, 1, 6, 2,\n",
       "       6, 3, 1, 2, 5, 6, 0, 1, 6, 4, 1, 6, 1, 1, 0, 0, 1, 1, 3, 5, 1, 4,\n",
       "       1, 0, 5, 2, 0, 1, 0, 2, 4, 0, 5, 4, 6, 6, 5, 6, 6, 0, 4, 2, 1, 6,\n",
       "       6, 3, 2, 1, 6, 0, 0, 6, 6, 0, 3, 0, 2, 0, 4, 4, 6, 0, 6, 1, 1, 1,\n",
       "       3, 7, 1, 7, 4, 1, 5, 0, 6, 6, 6, 1, 6, 2, 3, 1, 1, 6, 7, 1, 2, 6,\n",
       "       0, 6, 1, 3, 1, 6, 0, 0, 1, 1, 6, 2, 6, 3, 1, 2, 5, 6, 0, 1, 6, 4,\n",
       "       1, 6, 1, 1, 0, 0, 1, 1, 3, 5, 1, 4, 1, 0, 5, 2, 0, 1, 0, 2, 4, 0,\n",
       "       5, 4, 6, 6, 5, 6, 6, 0, 4, 2, 1, 6, 6, 3, 2, 1, 6, 0, 0, 6, 6, 0,\n",
       "       3, 0], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 逻辑回归模型\n",
    "调用sklearn.linear_model库的LogisticRegression方法实例化模型对象。  <br>\n",
    "调用sklearn.model_selection库的train_test_split方法划分训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_avg_f1(y_true, y_pred):\n",
    "    '''\n",
    "    分类 评估函数\n",
    "    :param y_true: 样本实际类别\n",
    "    :param y_pred: 预测类别\n",
    "    :return:\n",
    "    '''\n",
    "    return f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test= train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 1343)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现X_train 中在小量的数据集合中1343 维度，当数据比较大的时候，维度会极大增长，可以通过LR等模型进行特征抽取并特征融合，最终不同模型抽取\n",
    "的特征进行xgboost模型训练，可以起到很好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6166666666666667"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict_proba(X_test)\n",
    "y_pred_score = micro_avg_f1(y_test,lr.predict(X_test))\n",
    "y_pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25172941, 0.1163091 , 0.0441257 , 0.03587257, 0.04831882,\n",
       "        0.04172943, 0.42599942, 0.03591555],\n",
       "       [0.1112396 , 0.4791083 , 0.05380893, 0.0727586 , 0.06158119,\n",
       "        0.05658974, 0.12834826, 0.03656538],\n",
       "       [0.07822829, 0.55665559, 0.17920372, 0.03018259, 0.04340848,\n",
       "        0.02989947, 0.05318464, 0.02923722],\n",
       "       [0.20160887, 0.18495437, 0.07713558, 0.05319286, 0.15353556,\n",
       "        0.06819826, 0.21529086, 0.04608364],\n",
       "       [0.11231275, 0.12139835, 0.06456061, 0.04548143, 0.05955594,\n",
       "        0.07963159, 0.45002465, 0.06703468],\n",
       "       [0.17124143, 0.1770872 , 0.07022411, 0.04965321, 0.167869  ,\n",
       "        0.07659912, 0.24256407, 0.04476186],\n",
       "       [0.07473505, 0.57600465, 0.15502355, 0.03104757, 0.04422462,\n",
       "        0.03090738, 0.05836906, 0.02968811],\n",
       "       [0.16511812, 0.29619346, 0.06178228, 0.04482196, 0.06622193,\n",
       "        0.09717071, 0.22517938, 0.04351216],\n",
       "       [0.08905819, 0.52533818, 0.17355755, 0.03240536, 0.05166649,\n",
       "        0.03301305, 0.06320494, 0.03175625],\n",
       "       [0.07699396, 0.32323419, 0.136695  , 0.21546219, 0.08347743,\n",
       "        0.03084225, 0.10692521, 0.02636975],\n",
       "       [0.08027346, 0.54913353, 0.17430691, 0.03082657, 0.04593931,\n",
       "        0.03145509, 0.05756144, 0.0305037 ],\n",
       "       [0.0799667 , 0.53507207, 0.18826579, 0.03082777, 0.04527559,\n",
       "        0.03152661, 0.05832163, 0.03074384],\n",
       "       [0.36802907, 0.32216443, 0.06982062, 0.04747853, 0.04381723,\n",
       "        0.04223684, 0.07892036, 0.02753293],\n",
       "       [0.08062598, 0.55598324, 0.16968178, 0.03051906, 0.04458306,\n",
       "        0.03139573, 0.05727663, 0.02993452],\n",
       "       [0.07819414, 0.3068006 , 0.10120503, 0.04269653, 0.16778575,\n",
       "        0.073972  , 0.18788208, 0.04146387],\n",
       "       [0.2957625 , 0.32322241, 0.09536214, 0.06024844, 0.06349757,\n",
       "        0.03829069, 0.09281037, 0.03080589],\n",
       "       [0.11404735, 0.41043469, 0.08395817, 0.04214115, 0.0860238 ,\n",
       "        0.07458499, 0.15582433, 0.03298553],\n",
       "       [0.07279251, 0.26740686, 0.13368965, 0.26584183, 0.08337211,\n",
       "        0.031236  , 0.11991066, 0.02575037],\n",
       "       [0.15377492, 0.13692093, 0.07354336, 0.05363795, 0.07261404,\n",
       "        0.12521712, 0.33890164, 0.04539004],\n",
       "       [0.45600578, 0.21979485, 0.07985149, 0.04258577, 0.04402717,\n",
       "        0.04104513, 0.09142273, 0.02526707],\n",
       "       [0.07567093, 0.25491527, 0.15850068, 0.25315269, 0.09563485,\n",
       "        0.03132177, 0.10398457, 0.02681923],\n",
       "       [0.07209118, 0.58170699, 0.15770863, 0.03039804, 0.0431047 ,\n",
       "        0.02986392, 0.05607782, 0.02904872],\n",
       "       [0.07332845, 0.58262209, 0.15052107, 0.03050416, 0.04501787,\n",
       "        0.03059285, 0.05806638, 0.02934713],\n",
       "       [0.12519049, 0.23105779, 0.07786909, 0.04412617, 0.17390002,\n",
       "        0.0998045 , 0.2082943 , 0.03975763],\n",
       "       [0.13246101, 0.34321857, 0.05948643, 0.07088854, 0.06633052,\n",
       "        0.09326831, 0.19691545, 0.03743117],\n",
       "       [0.23860964, 0.15675329, 0.06833422, 0.05146803, 0.06510674,\n",
       "        0.11981496, 0.25652164, 0.04339148],\n",
       "       [0.07662835, 0.25020826, 0.17184866, 0.2178149 , 0.09255728,\n",
       "        0.03466089, 0.12769212, 0.02858955],\n",
       "       [0.3445818 , 0.12682236, 0.05551136, 0.04363521, 0.06135914,\n",
       "        0.04917797, 0.28299526, 0.03591689],\n",
       "       [0.47630784, 0.15890434, 0.05150286, 0.04107276, 0.05559926,\n",
       "        0.05138631, 0.1324932 , 0.03273342],\n",
       "       [0.07886224, 0.33939666, 0.14918716, 0.16996097, 0.0974389 ,\n",
       "        0.03015188, 0.10925972, 0.02574247],\n",
       "       [0.15377492, 0.13692093, 0.07354336, 0.05363795, 0.07261404,\n",
       "        0.12521712, 0.33890164, 0.04539004],\n",
       "       [0.10086733, 0.10932022, 0.05401283, 0.04561935, 0.06388083,\n",
       "        0.10523803, 0.48450774, 0.03655368],\n",
       "       [0.17437455, 0.1392071 , 0.0545856 , 0.04430383, 0.08062416,\n",
       "        0.23127468, 0.23703243, 0.03859766],\n",
       "       [0.13940552, 0.13817595, 0.06301406, 0.05521055, 0.10830972,\n",
       "        0.05296683, 0.39090213, 0.05201526],\n",
       "       [0.12540109, 0.48387773, 0.05406826, 0.08061494, 0.05252327,\n",
       "        0.04851404, 0.11461319, 0.04038748],\n",
       "       [0.08492708, 0.39179547, 0.09105735, 0.04195259, 0.09099494,\n",
       "        0.10250014, 0.15840428, 0.03836816],\n",
       "       [0.17290241, 0.13556437, 0.06218001, 0.05924737, 0.05501751,\n",
       "        0.0843891 , 0.39380182, 0.03689741],\n",
       "       [0.10375663, 0.30841151, 0.07486174, 0.03979183, 0.08706811,\n",
       "        0.07504003, 0.27443464, 0.03663551],\n",
       "       [0.42017663, 0.20338797, 0.07453952, 0.04721489, 0.04981842,\n",
       "        0.04928159, 0.12623645, 0.02934454],\n",
       "       [0.2957625 , 0.32322241, 0.09536214, 0.06024844, 0.06349757,\n",
       "        0.03829069, 0.09281037, 0.03080589],\n",
       "       [0.13594241, 0.42201529, 0.10070966, 0.04120209, 0.07198261,\n",
       "        0.08597193, 0.10887934, 0.03329668],\n",
       "       [0.06427286, 0.15420664, 0.12805551, 0.12533189, 0.07480941,\n",
       "        0.0546422 , 0.36135453, 0.03732697],\n",
       "       [0.15857631, 0.15733126, 0.06634974, 0.06053133, 0.11701773,\n",
       "        0.06236282, 0.31743839, 0.06039242],\n",
       "       [0.20160887, 0.18495437, 0.07713558, 0.05319286, 0.15353556,\n",
       "        0.06819826, 0.21529086, 0.04608364],\n",
       "       [0.23860964, 0.15675329, 0.06833422, 0.05146803, 0.06510674,\n",
       "        0.11981496, 0.25652164, 0.04339148],\n",
       "       [0.18470471, 0.18335629, 0.07450599, 0.04837644, 0.07957891,\n",
       "        0.20245955, 0.18790695, 0.03911118],\n",
       "       [0.12813619, 0.12371467, 0.05167152, 0.04204732, 0.05311452,\n",
       "        0.06033408, 0.50172844, 0.03925325],\n",
       "       [0.07173246, 0.1984032 , 0.12614834, 0.14950194, 0.08863449,\n",
       "        0.04858945, 0.28756388, 0.02942624],\n",
       "       [0.09393092, 0.0945816 , 0.04393353, 0.03935877, 0.04862505,\n",
       "        0.04904548, 0.59236172, 0.03816293],\n",
       "       [0.13179647, 0.10400038, 0.08905277, 0.05233801, 0.0563274 ,\n",
       "        0.07071736, 0.44417658, 0.05159103],\n",
       "       [0.0841937 , 0.09913674, 0.04567885, 0.03950459, 0.05149611,\n",
       "        0.04436038, 0.59991984, 0.03570978],\n",
       "       [0.0817787 , 0.20680599, 0.14265401, 0.15057579, 0.09546544,\n",
       "        0.05717353, 0.23152325, 0.03402329],\n",
       "       [0.08905819, 0.52533818, 0.17355755, 0.03240536, 0.05166649,\n",
       "        0.03301305, 0.06320494, 0.03175625],\n",
       "       [0.07209118, 0.58170699, 0.15770863, 0.03039804, 0.0431047 ,\n",
       "        0.02986392, 0.05607782, 0.02904872],\n",
       "       [0.15082723, 0.1640097 , 0.06536676, 0.05490255, 0.07864484,\n",
       "        0.10910644, 0.29653706, 0.08060541],\n",
       "       [0.47630784, 0.15890434, 0.05150286, 0.04107276, 0.05559926,\n",
       "        0.05138631, 0.1324932 , 0.03273342],\n",
       "       [0.0817787 , 0.20680599, 0.14265401, 0.15057579, 0.09546544,\n",
       "        0.05717353, 0.23152325, 0.03402329],\n",
       "       [0.15082723, 0.1640097 , 0.06536676, 0.05490255, 0.07864484,\n",
       "        0.10910644, 0.29653706, 0.08060541],\n",
       "       [0.08190951, 0.35012351, 0.14911673, 0.1750259 , 0.07921786,\n",
       "        0.03205054, 0.10662128, 0.02593467],\n",
       "       [0.08683611, 0.27783231, 0.18451255, 0.18936116, 0.0827539 ,\n",
       "        0.03463989, 0.11494699, 0.02911708]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6166666666666667"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 3 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=2, random_state=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 58  62  66  71  72  73  74  75  76  78  79  80  81  82  84  85  87  88\n",
      "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 59 60 61 63 64 65 67 68 69 70 77 83 86]\n",
      "Train_dataset...\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0 2 2 1 0 3 1 0 2 4 0 3 2 1 0 1 4 1 0 1 1 5 6 0 0 5 0 0 1 6 0 6 6 6 3 6 6\n",
      " 7 5 6 0 6 1 6 1 1 6 0 1 3 4 1 1 6 7 5 1 0 2 7 2 6 4 0 3 6 1 4 1]\n",
      "Validate_dataset...\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0 6 1 4 5 0 0 2 1 6 6 1 4 0 3 6 0 1 2 0 7 6 0 1 0 6 0 2 3 2 0 6 7 6 0 6 1\n",
      " 5 1 1 1 2 1 6 6 4 6 0 0 1 2 0 1 4 0 5 1 6 1 1 3 1 6 3 3 6 7 1 4 5 5]\n",
      "y_true y_pred.....\n",
      "[0 6 1 4 5 0 0 2 1 6 6 1 4 0 3 6 0 1 2 0 7 6 0 1 0 6 0 2 3 2 0 6 7 6 0 6 1\n",
      " 5 1 1 1 2 1 6 6 4 6 0 0 1 2 0 1 4 0 5 1 6 1 1 3 1 6 3 3 6 7 1 4 5 5] [1 1 1 1 1 0 1 0 1 6 6 1 0 0 6 1 1 1 1 0 6 6 0 0 0 6 0 1 1 1 0 6 6 6 1 6 1\n",
      " 0 0 1 6 1 1 0 0 1 6 1 1 1 0 0 1 0 1 6 1 6 1 1 1 1 6 1 6 6 6 0 1 1 6]\n",
      "y_pred_score=0.4507042253521127\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 59 60 61 63 64 65 67 68 69 70 77 83 86] TEST: [ 58  62  66  71  72  73  74  75  76  78  79  80  81  82  84  85  87  88\n",
      "  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139]\n",
      "Train_dataset...\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0 6 1 4 5 0 0 2 1 6 6 1 4 0 3 6 0 1 2 0 7 6 0 1 0 6 0 2 3 2 0 6 7 6 0 6 1\n",
      " 5 1 1 1 2 1 6 6 4 6 0 0 1 2 0 1 4 0 5 1 6 1 1 3 1 6 3 3 6 7 1 4 5 5]\n",
      "Validate_dataset...\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0 2 2 1 0 3 1 0 2 4 0 3 2 1 0 1 4 1 0 1 1 5 6 0 0 5 0 0 1 6 0 6 6 6 3 6 6\n",
      " 7 5 6 0 6 1 6 1 1 6 0 1 3 4 1 1 6 7 5 1 0 2 7 2 6 4 0 3 6 1 4 1]\n",
      "y_true y_pred.....\n",
      "[0 2 2 1 0 3 1 0 2 4 0 3 2 1 0 1 4 1 0 1 1 5 6 0 0 5 0 0 1 6 0 6 6 6 3 6 6\n",
      " 7 5 6 0 6 1 6 1 1 6 0 1 3 4 1 1 6 7 5 1 0 2 7 2 6 4 0 3 6 1 4 1] [1 1 6 1 0 1 0 0 1 0 0 1 6 1 0 1 1 0 1 1 1 6 6 0 0 6 1 0 1 6 6 6 6 6 3 6 6\n",
      " 6 6 6 0 6 1 6 1 0 6 1 1 1 0 1 1 6 1 1 1 0 1 1 1 6 6 0 3 6 0 1 1]\n",
      "y_pred_score=0.5652173913043478\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X_train, y_train):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_data,val_data=X_train[train_index],X_train[test_index]\n",
    "    train_y,val_y=y_train[train_index],y_train[test_index]\n",
    "    print(\"Train_dataset...\")\n",
    "    print(train_data.toarray(),train_y)\n",
    "    print(\"Validate_dataset...\")\n",
    "    print(val_data.toarray(),val_y)\n",
    "    lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    lr.fit(train_data, train_y)\n",
    "    y_pred_val=lr.predict(val_data)\n",
    "    print(\"y_true y_pred.....\")\n",
    "    print(val_y,y_pred_val)\n",
    "    y_pred_score = micro_avg_f1(val_y,y_pred_val)\n",
    "    y_pred_score\n",
    "    print(\"y_pred_score={}\".format(y_pred_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 基于模型的特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf 提供的原始样本后，然后通过LR。。。等模型提取 特征。  通过多个模型提取特征，进行特征融合，然后再进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(np.unique(train_df['penalty']))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化stack ： 通过模型提取特征矩阵的数值\n",
    "stack=np.zeros((X_train.shape[0],num_class))\n",
    "stack_te = np.zeros((X_test.shape[0], num_class))\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-25 17:29:34 val_dataset accuacy:0.450704\n",
      "2019-03-25 17:29:34 test_dataset accuacy:0.533333\n",
      "extract feature based lr.\n",
      "2019-03-25 17:29:34 val_dataset accuacy:0.565217\n",
      "2019-03-25 17:29:34 test_dataset accuacy:0.650000\n",
      "extract feature based lr.\n",
      "2019-03-25 17:29:34 val_dataset avg acc:0.507961\n",
      "2019-03-25 17:29:34 test_dataset avg acc:0.591667\n"
     ]
    }
   ],
   "source": [
    "k=2\n",
    "score_va = 0\n",
    "score_te = 0\n",
    "skf = StratifiedKFold(n_splits=k, random_state=config.seed)\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_data,val_data=X_train[train_index],X_train[val_index]\n",
    "    train_y,val_y=y_train[train_index],y_train[val_index]\n",
    "    lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    lr.fit(train_data, train_y)\n",
    "    y_pred_val=lr.predict(val_data)\n",
    "    val_accuracy = micro_avg_f1(val_y,y_pred_val)\n",
    "    test_accuacy = micro_avg_f1(y_test,lr.predict(X_test))\n",
    "    LOGGER.log('val_dataset accuacy:%f' % val_accuracy)\n",
    "    LOGGER.log('test_dataset accuacy:%f' % test_accuacy)\n",
    "    score_va += val_accuracy\n",
    "    score_te += test_accuacy\n",
    "    print(\"extract feature based lr.\")\n",
    "    y_pred_val_prob = lr.predict_proba(val_data)\n",
    "    y_pred_te_prob = lr.predict_proba(X_test)\n",
    "    stack[val_index]+=y_pred_val_prob\n",
    "    stack_te+=y_pred_te_prob\n",
    "score_va /= k\n",
    "score_te /= k\n",
    "LOGGER.log('val_dataset avg acc:%f' % score_va)\n",
    "LOGGER.log('test_dataset avg acc:%f' % score_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 8) 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.13563476, 0.13719248, 0.030327  , 0.0253883 , 0.05564552,\n",
       "        0.02432604, 0.06717233, 0.02431357],\n",
       "       [0.04900977, 0.12568846, 0.08445302, 0.05948503, 0.03772673,\n",
       "        0.02368232, 0.10294757, 0.01700711],\n",
       "       [0.09295997, 0.18761264, 0.02843153, 0.02509351, 0.03274937,\n",
       "        0.03849071, 0.07668153, 0.01798074],\n",
       "       [0.04623509, 0.15607241, 0.08523052, 0.07463287, 0.04312961,\n",
       "        0.01720253, 0.06239845, 0.01509852]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_all = np.vstack([stack/k,stack_te/k])\n",
    "print(stack_all.shape,stack_all.shape[1])\n",
    "stack_all[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stack = pd.DataFrame(index=range(len(train_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(stack_all.shape[1]):\n",
    "    df_stack['tfidf_lr_{}'.format(i)] = stack_all[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_lr_0</th>\n",
       "      <th>tfidf_lr_1</th>\n",
       "      <th>tfidf_lr_2</th>\n",
       "      <th>tfidf_lr_3</th>\n",
       "      <th>tfidf_lr_4</th>\n",
       "      <th>tfidf_lr_5</th>\n",
       "      <th>tfidf_lr_6</th>\n",
       "      <th>tfidf_lr_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135635</td>\n",
       "      <td>0.137192</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.055646</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.067172</td>\n",
       "      <td>0.024314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049010</td>\n",
       "      <td>0.125688</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>0.059485</td>\n",
       "      <td>0.037727</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.102948</td>\n",
       "      <td>0.017007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.092960</td>\n",
       "      <td>0.187613</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.032749</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.017981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046235</td>\n",
       "      <td>0.156072</td>\n",
       "      <td>0.085231</td>\n",
       "      <td>0.074633</td>\n",
       "      <td>0.043130</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>0.062398</td>\n",
       "      <td>0.015099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111274</td>\n",
       "      <td>0.111634</td>\n",
       "      <td>0.045155</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.039935</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.020703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.259456</td>\n",
       "      <td>0.070825</td>\n",
       "      <td>0.027218</td>\n",
       "      <td>0.019684</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.024704</td>\n",
       "      <td>0.058181</td>\n",
       "      <td>0.015329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.129580</td>\n",
       "      <td>0.149111</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.026943</td>\n",
       "      <td>0.032482</td>\n",
       "      <td>0.030487</td>\n",
       "      <td>0.082044</td>\n",
       "      <td>0.018853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.135493</td>\n",
       "      <td>0.122287</td>\n",
       "      <td>0.028288</td>\n",
       "      <td>0.024525</td>\n",
       "      <td>0.038407</td>\n",
       "      <td>0.031539</td>\n",
       "      <td>0.100964</td>\n",
       "      <td>0.018497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.058786</td>\n",
       "      <td>0.253379</td>\n",
       "      <td>0.049045</td>\n",
       "      <td>0.022862</td>\n",
       "      <td>0.025377</td>\n",
       "      <td>0.020135</td>\n",
       "      <td>0.049474</td>\n",
       "      <td>0.020943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.094105</td>\n",
       "      <td>0.071807</td>\n",
       "      <td>0.026306</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>0.033885</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>0.018126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_lr_0  tfidf_lr_1  tfidf_lr_2  tfidf_lr_3  tfidf_lr_4  tfidf_lr_5  \\\n",
       "0    0.135635    0.137192    0.030327    0.025388    0.055646    0.024326   \n",
       "1    0.049010    0.125688    0.084453    0.059485    0.037727    0.023682   \n",
       "2    0.092960    0.187613    0.028432    0.025094    0.032749    0.038491   \n",
       "3    0.046235    0.156072    0.085231    0.074633    0.043130    0.017203   \n",
       "4    0.111274    0.111634    0.045155    0.029082    0.037598    0.039935   \n",
       "5    0.259456    0.070825    0.027218    0.019684    0.024603    0.024704   \n",
       "6    0.129580    0.149111    0.030500    0.026943    0.032482    0.030487   \n",
       "7    0.135493    0.122287    0.028288    0.024525    0.038407    0.031539   \n",
       "8    0.058786    0.253379    0.049045    0.022862    0.025377    0.020135   \n",
       "9    0.094105    0.071807    0.026306    0.022724    0.033885    0.032913   \n",
       "\n",
       "   tfidf_lr_6  tfidf_lr_7  \n",
       "0    0.067172    0.024314  \n",
       "1    0.102948    0.017007  \n",
       "2    0.076682    0.017981  \n",
       "3    0.062398    0.015099  \n",
       "4    0.104618    0.020703  \n",
       "5    0.058181    0.015329  \n",
       "6    0.082044    0.018853  \n",
       "7    0.100964    0.018497  \n",
       "8    0.049474    0.020943  \n",
       "9    0.200135    0.018126  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stack.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保持lr-提取的特征\n",
    "config.feat_tfidf_lr_prob = 'D:/ML_Study/2017-CCF-BDCI-AIJudge/data/output/feature/tfidf/lr_prob_12w.csv'\n",
    "df_stack.to_csv(config.feat_tfidf_lr_prob, index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 基于LR模型的特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用pickle库的dump方法保存模型，需要2个参数。\n",
    "#第1个参数是保存的对象，可以为任意数据类型，因为有3个模型需要保存，所以下面代码第1个参数是字典。\n",
    "#第2个参数是保存的文件对象，数据类型为_io.BufferedWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 特征提取-stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class=8\n",
      "dataset size X_train=140,X_test=60,dim=1343\n"
     ]
    }
   ],
   "source": [
    "# 1. 获取类别-个数=新的特征维度\n",
    "num_class = len(np.unique(train_df['penalty']))\n",
    "print(\"num_class={0}\".format(num_class))\n",
    "# 2.初始化stack ： 通过模型提取特征矩阵的数值\n",
    "print(\"dataset size X_train={0},X_test={1},dim={2}\".format(X_train.shape[0],X_test.shape[0],X_test.shape[1]))\n",
    "stack=np.zeros((X_train.shape[0],num_class))\n",
    "stack_te = np.zeros((X_test.shape[0], num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 lr 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate k=2\n",
      "cross_validate.....i=1\n",
      "2019-03-25 17:29:41 val_dataset accuacy:0.450704\n",
      "2019-03-25 17:29:41 test_dataset accuacy:0.533333\n",
      "cross_validate.....i=2\n",
      "2019-03-25 17:29:41 val_dataset accuacy:0.565217\n",
      "2019-03-25 17:29:41 test_dataset accuacy:0.650000\n",
      "cross_validate success. print avg acc ...\n",
      "2019-03-25 17:29:41 val_dataset avg acc:0.507961\n",
      "2019-03-25 17:29:41 test_dataset avg acc:0.591667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_lr_0</th>\n",
       "      <th>tfidf_lr_1</th>\n",
       "      <th>tfidf_lr_2</th>\n",
       "      <th>tfidf_lr_3</th>\n",
       "      <th>tfidf_lr_4</th>\n",
       "      <th>tfidf_lr_5</th>\n",
       "      <th>tfidf_lr_6</th>\n",
       "      <th>tfidf_lr_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135635</td>\n",
       "      <td>0.137192</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.055646</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.067172</td>\n",
       "      <td>0.024314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049010</td>\n",
       "      <td>0.125688</td>\n",
       "      <td>0.084453</td>\n",
       "      <td>0.059485</td>\n",
       "      <td>0.037727</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>0.102948</td>\n",
       "      <td>0.017007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.092960</td>\n",
       "      <td>0.187613</td>\n",
       "      <td>0.028432</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>0.032749</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.076682</td>\n",
       "      <td>0.017981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046235</td>\n",
       "      <td>0.156072</td>\n",
       "      <td>0.085231</td>\n",
       "      <td>0.074633</td>\n",
       "      <td>0.043130</td>\n",
       "      <td>0.017203</td>\n",
       "      <td>0.062398</td>\n",
       "      <td>0.015099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111274</td>\n",
       "      <td>0.111634</td>\n",
       "      <td>0.045155</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>0.039935</td>\n",
       "      <td>0.104618</td>\n",
       "      <td>0.020703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_lr_0  tfidf_lr_1  tfidf_lr_2  tfidf_lr_3  tfidf_lr_4  tfidf_lr_5  \\\n",
       "0    0.135635    0.137192    0.030327    0.025388    0.055646    0.024326   \n",
       "1    0.049010    0.125688    0.084453    0.059485    0.037727    0.023682   \n",
       "2    0.092960    0.187613    0.028432    0.025094    0.032749    0.038491   \n",
       "3    0.046235    0.156072    0.085231    0.074633    0.043130    0.017203   \n",
       "4    0.111274    0.111634    0.045155    0.029082    0.037598    0.039935   \n",
       "\n",
       "   tfidf_lr_6  tfidf_lr_7  \n",
       "0    0.067172    0.024314  \n",
       "1    0.102948    0.017007  \n",
       "2    0.076682    0.017981  \n",
       "3    0.062398    0.015099  \n",
       "4    0.104618    0.020703  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 交叉验证- 特征得分平均\n",
    "k=2 \n",
    "print(\"cross_validate k={0}\".format(k))\n",
    "score_va = 0\n",
    "score_te = 0\n",
    "skf = StratifiedKFold(n_splits=k, random_state=config.seed)\n",
    "for i,(train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(\"cross_validate.....i={0}\".format(i+1))\n",
    "    train_data,val_data=X_train[train_index],X_train[val_index]\n",
    "    train_y,val_y=y_train[train_index],y_train[val_index]\n",
    "    clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    clf.fit(train_data, train_y)\n",
    "    y_pred_val=clf.predict(val_data)\n",
    "    val_accuracy = micro_avg_f1(val_y,y_pred_val)\n",
    "    test_accuacy = micro_avg_f1(y_test,clf.predict(X_test))\n",
    "    LOGGER.log('val_dataset accuacy:%f' % val_accuracy)\n",
    "    LOGGER.log('test_dataset accuacy:%f' % test_accuacy)\n",
    "    score_va += val_accuracy\n",
    "    score_te += test_accuacy\n",
    "    y_pred_val_prob = clf.predict_proba(val_data)\n",
    "    y_pred_te_prob = clf.predict_proba(X_test)\n",
    "    stack[val_index]+=y_pred_val_prob\n",
    "    stack_te+=y_pred_te_prob\n",
    "score_va /= k\n",
    "score_te /= k\n",
    "print(\"cross_validate success. print avg acc ...\")\n",
    "LOGGER.log('val_dataset avg acc:%f' % score_va)\n",
    "LOGGER.log('test_dataset avg acc:%f' % score_te)\n",
    "# 2. lr... 提取的特征存储\n",
    "stack_all = np.vstack([stack/k,stack_te/k])\n",
    "df_stack = pd.DataFrame(index=range(len(train_df)))\n",
    "for i in range(stack_all.shape[1]):\n",
    "    df_stack['tfidf_lr_{}'.format(i)] = stack_all[:, i]\n",
    "\n",
    "config.feat_tfidf_lr_prob = 'D:/ML_Study/2017-CCF-BDCI-AIJudge/data/output/feature/tfidf/lr_prob_12w.csv'\n",
    "df_stack.to_csv(config.feat_tfidf_lr_prob, index=None, encoding='utf8')\n",
    "df_stack.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 BernoulliNB 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate k=5\n",
      "cross_validate.....i=1\n",
      "2019-03-25 17:29:58 val_dataset accuacy:0.466667\n",
      "2019-03-25 17:29:58 test_dataset accuacy:0.600000\n",
      "cross_validate.....i=2\n",
      "2019-03-25 17:29:58 val_dataset accuacy:0.482759\n",
      "2019-03-25 17:29:58 test_dataset accuacy:0.600000\n",
      "cross_validate.....i=3\n",
      "2019-03-25 17:29:58 val_dataset accuacy:0.535714\n",
      "2019-03-25 17:29:58 test_dataset accuacy:0.566667\n",
      "cross_validate.....i=4\n",
      "2019-03-25 17:29:58 val_dataset accuacy:0.607143\n",
      "2019-03-25 17:29:58 test_dataset accuacy:0.616667\n",
      "cross_validate.....i=5\n",
      "2019-03-25 17:29:58 val_dataset accuacy:0.520000\n",
      "2019-03-25 17:29:58 test_dataset accuacy:0.550000\n",
      "cross_validate success. print avg acc ...\n",
      "2019-03-25 17:29:58 val_dataset avg acc:0.522456\n",
      "2019-03-25 17:29:58 test_dataset avg acc:0.586667\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'feat_tfidf_bnb_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-50566eff2e1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeat_tfidf_gnb_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/ML_Study/2017-CCF-BDCI-AIJudge/data/output/feature/tfidf/bnb_prob_12w.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mdf_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeat_tfidf_bnb_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mdf_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Config' object has no attribute 'feat_tfidf_bnb_prob'"
     ]
    }
   ],
   "source": [
    "# 1. 交叉验证- 特征得分平均\n",
    "k=5 \n",
    "print(\"cross_validate k={0}\".format(k))\n",
    "score_va = 0\n",
    "score_te = 0\n",
    "skf = StratifiedKFold(n_splits=k, random_state=config.seed)\n",
    "for i,(train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(\"cross_validate.....i={0}\".format(i+1))\n",
    "    train_data,val_data=X_train[train_index],X_train[val_index]\n",
    "    train_y,val_y=y_train[train_index],y_train[val_index]\n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(train_data, train_y)\n",
    "    y_pred_val=clf.predict(val_data)\n",
    "    val_accuracy = micro_avg_f1(val_y,y_pred_val)\n",
    "    test_accuacy = micro_avg_f1(y_test,clf.predict(X_test))\n",
    "    LOGGER.log('val_dataset accuacy:%f' % val_accuracy)\n",
    "    LOGGER.log('test_dataset accuacy:%f' % test_accuacy)\n",
    "    score_va += val_accuracy\n",
    "    score_te += test_accuacy\n",
    "    y_pred_val_prob = clf.predict_proba(val_data)\n",
    "    y_pred_te_prob = clf.predict_proba(X_test)\n",
    "    stack[val_index]+=y_pred_val_prob\n",
    "    stack_te+=y_pred_te_prob\n",
    "score_va /= k\n",
    "score_te /= k\n",
    "print(\"cross_validate success. print avg acc ...\")\n",
    "LOGGER.log('val_dataset avg acc:%f' % score_va)\n",
    "LOGGER.log('test_dataset avg acc:%f' % score_te)\n",
    "# 2. lr... 提取的特征存储\n",
    "stack_all = np.vstack([stack/k,stack_te/k])\n",
    "df_stack = pd.DataFrame(index=range(len(train_df)))\n",
    "for i in range(stack_all.shape[1]):\n",
    "    df_stack['tfidf_bnb_{}'.format(i)] = stack_all[:, i]\n",
    "\n",
    "config.feat_tfidf_gnb_prob = 'D:/ML_Study/2017-CCF-BDCI-AIJudge/data/output/feature/tfidf/bnb_prob_12w.csv'\n",
    "df_stack.to_csv(config.feat_tfidf_bnb_prob, index=None, encoding='utf8')\n",
    "df_stack.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3  MultinomialNB 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate k=2\n",
      "cross_validate.....i=1\n",
      "2019-03-25 16:00:43 val_dataset accuacy:0.630137\n",
      "2019-03-25 16:00:43 test_dataset accuacy:0.416667\n",
      "cross_validate.....i=2\n",
      "2019-03-25 16:00:43 val_dataset accuacy:0.597015\n",
      "2019-03-25 16:00:43 test_dataset accuacy:0.383333\n",
      "cross_validate success. print avg acc ...\n",
      "2019-03-25 16:00:43 val_dataset avg acc:0.613576\n",
      "2019-03-25 16:00:43 test_dataset avg acc:0.400000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_mnb_0</th>\n",
       "      <th>tfidf_mnb_1</th>\n",
       "      <th>tfidf_mnb_2</th>\n",
       "      <th>tfidf_mnb_3</th>\n",
       "      <th>tfidf_mnb_4</th>\n",
       "      <th>tfidf_mnb_5</th>\n",
       "      <th>tfidf_mnb_6</th>\n",
       "      <th>tfidf_mnb_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.661676</td>\n",
       "      <td>0.686872</td>\n",
       "      <td>0.199022</td>\n",
       "      <td>0.098458</td>\n",
       "      <td>0.100374</td>\n",
       "      <td>0.066655</td>\n",
       "      <td>0.585861</td>\n",
       "      <td>0.101082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.934575</td>\n",
       "      <td>2.129631</td>\n",
       "      <td>0.260996</td>\n",
       "      <td>0.183068</td>\n",
       "      <td>0.144609</td>\n",
       "      <td>0.098875</td>\n",
       "      <td>2.613859</td>\n",
       "      <td>0.134388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.613455</td>\n",
       "      <td>1.747695</td>\n",
       "      <td>0.205373</td>\n",
       "      <td>0.136668</td>\n",
       "      <td>0.121855</td>\n",
       "      <td>0.097442</td>\n",
       "      <td>1.461603</td>\n",
       "      <td>0.115910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.454932</td>\n",
       "      <td>0.755420</td>\n",
       "      <td>0.310109</td>\n",
       "      <td>0.129176</td>\n",
       "      <td>0.135088</td>\n",
       "      <td>0.087560</td>\n",
       "      <td>4.499000</td>\n",
       "      <td>0.128714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.414802</td>\n",
       "      <td>4.042746</td>\n",
       "      <td>0.625174</td>\n",
       "      <td>0.098356</td>\n",
       "      <td>0.086652</td>\n",
       "      <td>0.058404</td>\n",
       "      <td>1.089347</td>\n",
       "      <td>0.084519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_mnb_0  tfidf_mnb_1  tfidf_mnb_2  tfidf_mnb_3  tfidf_mnb_4  \\\n",
       "0     4.661676     0.686872     0.199022     0.098458     0.100374   \n",
       "1     0.934575     2.129631     0.260996     0.183068     0.144609   \n",
       "2     2.613455     1.747695     0.205373     0.136668     0.121855   \n",
       "3     0.454932     0.755420     0.310109     0.129176     0.135088   \n",
       "4     0.414802     4.042746     0.625174     0.098356     0.086652   \n",
       "\n",
       "   tfidf_mnb_5  tfidf_mnb_6  tfidf_mnb_7  \n",
       "0     0.066655     0.585861     0.101082  \n",
       "1     0.098875     2.613859     0.134388  \n",
       "2     0.097442     1.461603     0.115910  \n",
       "3     0.087560     4.499000     0.128714  \n",
       "4     0.058404     1.089347     0.084519  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 交叉验证- 特征得分平均\n",
    "k=2 \n",
    "print(\"cross_validate k={0}\".format(k))\n",
    "score_va = 0\n",
    "score_te = 0\n",
    "skf = StratifiedKFold(n_splits=k, random_state=config.seed)\n",
    "for i,(train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    print(\"cross_validate.....i={0}\".format(i+1))\n",
    "    train_data,val_data=X_train[train_index],X_train[val_index]\n",
    "    train_y,val_y=y_train[train_index],y_train[val_index]\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(train_data, train_y)\n",
    "    y_pred_val=clf.predict(val_data)\n",
    "    val_accuracy = micro_avg_f1(val_y,y_pred_val)\n",
    "    test_accuacy = micro_avg_f1(y_test,clf.predict(X_test))\n",
    "    LOGGER.log('val_dataset accuacy:%f' % val_accuracy)\n",
    "    LOGGER.log('test_dataset accuacy:%f' % test_accuacy)\n",
    "    score_va += val_accuracy\n",
    "    score_te += test_accuacy\n",
    "    y_pred_val_prob = clf.predict_proba(val_data)\n",
    "    y_pred_te_prob = clf.predict_proba(X_test)\n",
    "    stack[val_index]+=y_pred_val_prob\n",
    "    stack_te+=y_pred_te_prob\n",
    "score_va /= k\n",
    "score_te /= k\n",
    "print(\"cross_validate success. print avg acc ...\")\n",
    "LOGGER.log('val_dataset avg acc:%f' % score_va)\n",
    "LOGGER.log('test_dataset avg acc:%f' % score_te)\n",
    "# 2. lr... 提取的特征存储\n",
    "stack_all = np.vstack([stack/k,stack_te/k])\n",
    "df_stack = pd.DataFrame(index=range(len(train_df)))\n",
    "for i in range(stack_all.shape[1]):\n",
    "    df_stack['tfidf_mnb_{}'.format(i)] = stack_all[:, i]\n",
    "\n",
    "config.feat_tfidf_mnb_prob = 'D:/ML_Study/2017-CCF-BDCI-AIJudge/data/output/feature/tfidf/mnb_prob_12w.csv'\n",
    "df_stack.to_csv(config.feat_tfidf_mnb_prob, index=None, encoding='utf8')\n",
    "df_stack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
